# VoiceGuard â€“ Dockerized Inference Environment

## ğŸ“Œ Overview

This project implements a **Dockerized Machine Learning Inference API** using **FastAPI**.
The application exposes REST endpoints for **health checks** and **real-time predictions**, with the ML model loaded safely inside the container.

The goal of this task is to demonstrate the ability to **containerize an inference service** with **clear build and run instructions**, suitable for production and MLOps workflows.

---

## ğŸ› ï¸ Tech Stack

- **Programming Language:** Python 3.10  
- **Web Framework:** FastAPI  
- **ASGI Server:** Uvicorn  
- **Containerization:** Docker  
- **Base Image:** python:3.10-slim  
- **API Documentation:** Swagger UI (OpenAPI)  
- **Deployment Platform:** AWS EC2 (Dockerized)
---

## ğŸ“‚ Project Structure

```
.
fastapi-docker/
â”‚â”€â”€ app/
â”‚ â””â”€â”€ main.py
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ Dockerfile
â”‚â”€â”€ README.md
```


## ğŸ³ Docker Build & Run Instructions

### Build Docker Image

```bash
docker build -t fastapi-inference .
```

### Run Container

```bash
docker run -d -p 8000:80 fastapi-inference
```
Container listens on port 80

Application is accessible on port 8000 of the host

### Access the Application

Swagger UI
```bash
http://publicip:8000/docs
```
---

## ğŸ§ª Testing the Service


### Health Check

```
GET /health
```

Response:

```json
{
  "status": "ok"
}
```

### Prediction Endpoint

```
POST /predict
```

Request:

```json
{
  "text": "This is a good example"
}
```

Response:

```json
{
  "prediction": "positive",
  "confidence": 0.9
}
```
---

## â˜ï¸ Deployment Notes (AWS EC2)

* Allow inbound traffic on port **8000** in the EC2 Security Group
* Docker container exposes port **80**
* Uvicorn is bound to `0.0.0.0`

---

## âš™ï¸ Key Implementation Details

* The ML model is loaded **lazily** to prevent startup crashes
* Docker image uses a **lightweight Python base image**
* API is designed to be **deployment-ready** (Docker, CI/CD, Kubernetes)
* Health endpoint enables monitoring and orchestration readiness

---

## âœ… Task Status

âœ” **Task Completed Successfully**
âœ” Inference API containerized and tested
âœ” Clear build and run instructions provided

---

## ğŸ“ˆ Future Enhancements

* Kubernetes Deployment & Service
* CI/CD pipeline integration
* Model versioning
* Authentication and request validation

---

## ğŸ‘¤ Author

**Sakshith Reddy**
Dockerized Inference Environment â€“ VoiceGuard Task
